#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»„ç»‡åŒ–å¾®åšçˆ¬è™« - æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•ç»“æ„
"""

import ssl
import urllib.request
import urllib.parse
import json
import re
import os
import hashlib
from datetime import datetime, timedelta
import codecs


class OrganizedWeiboScraper:
    def __init__(self, output_base_dir="weibo_output"):
        # SSLè®¾ç½®
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.check_hostname = False
        self.ssl_context.verify_mode = ssl.CERT_NONE
        
        self.uid = "1317335037"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': f'https://m.weibo.cn/u/{self.uid}',
            'Connection': 'keep-alive',
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.output_base = output_base_dir
        self.reports_dir = os.path.join(self.output_base, "reports")
        self.images_dir = os.path.join(self.output_base, "images")
        self.data_dir = os.path.join(self.output_base, "data")
        
        # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•
        for directory in [self.output_base, self.reports_dir, self.images_dir, self.data_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)
                print(f"ğŸ“ åˆ›å»ºç›®å½•: {directory}")

    def decode_text_properly(self, text):
        """æ­£ç¡®è§£ç Unicodeæ–‡æœ¬"""
        if not text:
            return ""
        
        try:
            if isinstance(text, str):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«Unicodeè½¬ä¹‰åºåˆ—
                if '\\u' in text:
                    try:
                        decoded = json.loads(f'"{text}"')
                        return decoded
                    except:
                        try:
                            decoded = codecs.decode(text, 'unicode_escape')
                            return decoded
                        except:
                            pass
                return text
            return str(text)
        except Exception as e:
            return str(text) if text else ""

    def clean_html_and_decode(self, text):
        """æ¸…ç†HTMLå¹¶è§£ç """
        if not text:
            return ""
        
        # å…ˆæ¸…ç†HTMLæ ‡ç­¾
        text = re.sub(r'<br\s*/?>', '\n', text)
        text = re.sub(r'<[^>]+>', '', text)
        
        # è§£ç Unicode
        text = self.decode_text_properly(text)
        
        # æ¸…ç†HTMLå®ä½“
        html_entities = {
            '&nbsp;': ' ',
            '&amp;': '&',
            '&lt;': '<',
            '&gt;': '>',
            '&quot;': '"',
            '&#39;': "'",
            '&hellip;': 'â€¦'
        }
        
        for entity, char in html_entities.items():
            text = text.replace(entity, char)
        
        return text.strip()

    def make_request(self, url, max_retries=3):
        """å‘é€HTTPè¯·æ±‚"""
        for attempt in range(max_retries):
            try:
                req = urllib.request.Request(url, headers=self.headers)
                response = urllib.request.urlopen(req, timeout=30, context=self.ssl_context)
                
                content_bytes = response.read()
                try:
                    content = content_bytes.decode('utf-8')
                except UnicodeDecodeError:
                    try:
                        content = content_bytes.decode('gbk')
                    except UnicodeDecodeError:
                        content = content_bytes.decode('utf-8', errors='ignore')
                
                return content
                
            except Exception as e:
                if attempt < max_retries - 1:
                    import time
                    time.sleep(2)
                    
        return None

    def get_full_text(self, weibo_id):
        """è·å–å¾®åšå…¨æ–‡"""
        full_text_url = f"https://m.weibo.cn/statuses/extend?id={weibo_id}"
        
        try:
            content = self.make_request(full_text_url)
            if content:
                data = json.loads(content)
                if data.get('ok') == 1:
                    full_text = data.get('data', {}).get('longTextContent', '')
                    if full_text:
                        return self.clean_html_and_decode(full_text)
        except Exception as e:
            print(f"  è·å–å…¨æ–‡å¤±è´¥: {e}")
        
        return None

    def download_image(self, image_url, weibo_id, image_index):
        """ä¸‹è½½å›¾ç‰‡åˆ°imagesç›®å½•"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            image_ext = image_url.split('.')[-1].split('?')[0]
            if not image_ext or image_ext not in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                image_ext = 'jpg'
            
            filename = f"{weibo_id}_{image_index}.{image_ext}"
            filepath = os.path.join(self.images_dir, filename)
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
            if os.path.exists(filepath):
                return filename
            
            # ä¸‹è½½å›¾ç‰‡
            req = urllib.request.Request(image_url, headers={
                'User-Agent': self.headers['User-Agent'],
                'Referer': 'https://weibo.com/'
            })
            
            response = urllib.request.urlopen(req, timeout=30, context=self.ssl_context)
            
            with open(filepath, 'wb') as f:
                f.write(response.read())
            
            print(f"    âœ… ä¸‹è½½å›¾ç‰‡: images/{filename}")
            return filename
            
        except Exception as e:
            print(f"    âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {image_url} - {e}")
            return None

    def generate_weibo_url(self, weibo_id, mid=None):
        """ç”Ÿæˆå¾®åšé“¾æ¥"""
        if mid:
            return f"https://weibo.com/{self.uid}/{mid}"
        else:
            return f"https://m.weibo.cn/detail/{weibo_id}"

    def parse_weibo_date(self, date_str):
        """è§£æå¾®åšæ—¶é—´"""
        try:
            date_str = re.sub(r'\s+\+\d{4}', '', date_str)
            
            months = {
                'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
                'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12
            }
            
            parts = date_str.split()
            if len(parts) >= 5:
                month_str = parts[1]
                day = int(parts[2])
                time_str = parts[3]
                year = int(parts[4])
                
                if month_str in months:
                    month = months[month_str]
                    hour, minute, second = map(int, time_str.split(':'))
                    return datetime(year, month, day, hour, minute, second)
        except:
            pass
        
        return None

    def is_target_period(self, post_date):
        """æ£€æŸ¥æ˜¯å¦åœ¨ç›®æ ‡æ—¶é—´èŒƒå›´"""
        if not post_date:
            return False
        
        start_date = datetime(2025, 3, 1)
        end_date = datetime(2025, 9, 30, 23, 59, 59)
        return start_date <= post_date <= end_date

    def extract_weibo(self, mblog):
        """æå–å¾®åšæ•°æ®"""
        try:
            # è·å–åŸºæœ¬ä¿¡æ¯
            weibo_id = mblog.get('id', '')
            mid = mblog.get('mid', '')
            
            # è§£ææ—¶é—´
            created_at = mblog.get('created_at', '')
            post_date = self.parse_weibo_date(created_at)
            
            if not self.is_target_period(post_date):
                return None
            
            # è·å–æ–‡æœ¬å†…å®¹
            raw_text = mblog.get('text', '')
            clean_text = self.clean_html_and_decode(raw_text)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å…¨æ–‡
            is_long_text = mblog.get('isLongText', False)
            if is_long_text or 'å…¨æ–‡' in clean_text:
                print(f"    ğŸ” æ£€æµ‹åˆ°é•¿æ–‡æœ¬ï¼Œè·å–å…¨æ–‡...")
                full_text = self.get_full_text(weibo_id)
                if full_text:
                    clean_text = full_text
                    print(f"    âœ… è·å–å…¨æ–‡æˆåŠŸ: {len(full_text)} å­—ç¬¦")
            
            weibo = {
                'id': weibo_id,
                'mid': mid,
                'created_at': created_at,
                'parsed_date': post_date,
                'formatted_date': post_date.strftime('%Y-%m-%d %H:%M:%S') if post_date else '',
                'text': clean_text,
                'source': self.clean_html_and_decode(mblog.get('source', '')),
                'reposts_count': mblog.get('reposts_count', 0),
                'comments_count': mblog.get('comments_count', 0),
                'attitudes_count': mblog.get('attitudes_count', 0),
                'url': self.generate_weibo_url(weibo_id, mid),
            }
            
            # å¤„ç†å›¾ç‰‡
            if 'pics' in mblog and mblog['pics']:
                print(f"    ğŸ–¼ï¸ å‘ç° {len(mblog['pics'])} å¼ å›¾ç‰‡ï¼Œå¼€å§‹ä¸‹è½½...")
                weibo['images'] = []
                for idx, pic in enumerate(mblog['pics'], 1):
                    pic_url = pic.get('large', {}).get('url', '') or pic.get('url', '')
                    if pic_url:
                        # ä¸‹è½½å›¾ç‰‡
                        local_filename = self.download_image(pic_url, weibo_id, idx)
                        weibo['images'].append({
                            'url': pic_url,
                            'local_file': local_filename
                        })
            
            # å¤„ç†è½¬å‘å†…å®¹
            if 'retweeted_status' in mblog:
                rt = mblog['retweeted_status']
                rt_text = self.clean_html_and_decode(rt.get('text', ''))
                
                # æ£€æŸ¥è½¬å‘å†…å®¹æ˜¯å¦ä¹Ÿæœ‰å…¨æ–‡
                rt_id = rt.get('id', '')
                if rt.get('isLongText', False) or 'å…¨æ–‡' in rt_text:
                    print(f"    ğŸ” è½¬å‘å†…å®¹æ£€æµ‹åˆ°é•¿æ–‡æœ¬...")
                    rt_full_text = self.get_full_text(rt_id)
                    if rt_full_text:
                        rt_text = rt_full_text
                
                weibo['retweeted'] = {
                    'user_name': rt.get('user', {}).get('screen_name', ''),
                    'text': rt_text,
                    'id': rt_id,
                }
                
                # å¤„ç†è½¬å‘å†…å®¹çš„å›¾ç‰‡
                if 'pics' in rt and rt['pics']:
                    print(f"    ğŸ–¼ï¸ è½¬å‘å†…å®¹å‘ç° {len(rt['pics'])} å¼ å›¾ç‰‡...")
                    if 'images' not in weibo:
                        weibo['images'] = []
                    
                    for idx, pic in enumerate(rt['pics'], len(weibo.get('images', [])) + 1):
                        pic_url = pic.get('large', {}).get('url', '') or pic.get('url', '')
                        if pic_url:
                            local_filename = self.download_image(pic_url, f"{weibo_id}_rt", idx)
                            weibo['images'].append({
                                'url': pic_url,
                                'local_file': local_filename,
                                'from_retweet': True
                            })
            
            return weibo
            
        except Exception as e:
            print(f"  âŒ æå–å¾®åšå¤±è´¥: {e}")
            return None

    def scrape_all_pages(self):
        """çˆ¬å–æ‰€æœ‰é¡µé¢"""
        container_id = "1076031317335037"
        all_weibos = []
        
        print(f"ğŸ” å¼€å§‹çˆ¬å–ç”¨æˆ· {self.uid} çš„å®Œæ•´å¾®åšå†…å®¹...")
        
        for page in range(1, 11):  # å…ˆçˆ¬10é¡µæµ‹è¯•
            print(f"\nğŸ“„ çˆ¬å–ç¬¬ {page} é¡µ...")
            
            url = f"https://m.weibo.cn/api/container/getIndex?type=uid&value={self.uid}&containerid={container_id}&page={page}"
            
            content = self.make_request(url)
            if not content:
                print(f"âŒ ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥")
                break
            
            try:
                data = json.loads(content)
                
                if data.get('ok') != 1:
                    print(f"âŒ APIé”™è¯¯: {data.get('msg', 'æœªçŸ¥')}")
                    break
                
                cards = data.get('data', {}).get('cards', [])
                if not cards:
                    print(f"âš ï¸ ç¬¬ {page} é¡µæ— æ•°æ®")
                    break
                
                page_weibos = []
                for card in cards:
                    if card.get('card_type') == 9:
                        mblog = card.get('mblog')
                        if mblog:
                            print(f"  ğŸ”„ å¤„ç†å¾®åš ID: {mblog.get('id', '')}")
                            weibo = self.extract_weibo(mblog)
                            if weibo:
                                page_weibos.append(weibo)
                                print(f"    âœ… æˆåŠŸ: {weibo['formatted_date']}")
                                print(f"    ğŸ“ å†…å®¹: {weibo['text'][:100]}...")
                
                if page_weibos:
                    all_weibos.extend(page_weibos)
                    print(f"ğŸ“Š ç¬¬ {page} é¡µè·å– {len(page_weibos)} æ¡å¾®åš")
                else:
                    print(f"âš ï¸ ç¬¬ {page} é¡µæ— ç›®æ ‡å¾®åš")
                
                import time
                time.sleep(3)  # é¡µé¢é—´å»¶è¿Ÿ
                
            except Exception as e:
                print(f"âŒ å¤„ç†ç¬¬ {page} é¡µå¤±è´¥: {e}")
                break
        
        return all_weibos

    def save_data_json(self, weibos, filename):
        """ä¿å­˜åŸå§‹æ•°æ®ä¸ºJSONæ ¼å¼åˆ°dataç›®å½•"""
        filepath = os.path.join(self.data_dir, filename)
        
        data = {
            'user_id': self.uid,
            'user_name': 'å§œæ±ç¥¥-',
            'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_count': len(weibos),
            'weibos': weibos
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ åŸå§‹æ•°æ®å·²ä¿å­˜: data/{filename}")

    def generate_complete_markdown(self, weibos):
        """ç”Ÿæˆå®Œæ•´çš„Markdownåˆ°reportsç›®å½•"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"å§œæ±ç¥¥_å®Œæ•´å¾®åšæŠ¥å‘Š_{timestamp}.md"
        filepath = os.path.join(self.reports_dir, filename)
        
        # æŒ‰æ—¶é—´æ’åº
        weibos.sort(key=lambda x: x.get('parsed_date', datetime.min), reverse=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write("# å§œæ±ç¥¥- 2025å¹´3-9æœˆå®Œæ•´å¾®åšå†…å®¹\n\n")
            f.write("*âœ… åŒ…å«å…¨æ–‡å±•å¼€ã€å›¾ç‰‡ä¸‹è½½ã€å¾®åšé“¾æ¥çš„å®Œæ•´ç‰ˆæœ¬*\n\n")
            
            f.write("## ğŸ“ æ–‡ä»¶ç»„ç»‡ç»“æ„\n\n")
            f.write("```\n")
            f.write(f"{self.output_base}/\n")
            f.write("â”œâ”€â”€ reports/           # å¾®åšæŠ¥å‘Šæ–‡æ¡£\n")
            f.write("â”œâ”€â”€ images/            # ä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶\n")
            f.write("â”œâ”€â”€ data/              # åŸå§‹JSONæ•°æ®\n")
            f.write("â””â”€â”€ ...\n")
            f.write("```\n\n")
            
            f.write("## ğŸ“Š æ•°æ®ç»Ÿè®¡\n\n")
            f.write(f"- **ç”¨æˆ·**: å§œæ±ç¥¥- (èŒåœºåšä¸»)\n")
            f.write(f"- **å¾®åšæ€»æ•°**: {len(weibos)} æ¡\n")
            f.write(f"- **æ—¶é—´èŒƒå›´**: 2025å¹´3æœˆ-9æœˆ\n")
            f.write(f"- **æ•°æ®è·å–**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"- **ç‰¹è‰²åŠŸèƒ½**: âœ… å…¨æ–‡å±•å¼€ âœ… å›¾ç‰‡ä¸‹è½½ âœ… å¾®åšé“¾æ¥\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            if weibos:
                total_reposts = sum(w.get('reposts_count', 0) for w in weibos)
                total_comments = sum(w.get('comments_count', 0) for w in weibos)
                total_likes = sum(w.get('attitudes_count', 0) for w in weibos)
                total_images = sum(len(w.get('images', [])) for w in weibos)
                
                f.write("### ğŸ“ˆ ç»Ÿè®¡æ•°æ®\n")
                f.write(f"- **æ€»è½¬å‘æ•°**: {total_reposts:,}\n")
                f.write(f"- **æ€»è¯„è®ºæ•°**: {total_comments:,}\n") 
                f.write(f"- **æ€»ç‚¹èµæ•°**: {total_likes:,}\n")
                f.write(f"- **æ€»å›¾ç‰‡æ•°**: {total_images} å¼ \n")
                f.write(f"- **å¹³å‡äº’åŠ¨**: è½¬å‘{total_reposts/len(weibos):.1f} è¯„è®º{total_comments/len(weibos):.1f} ç‚¹èµ{total_likes/len(weibos):.1f}\n\n")
                
                f.write("---\n\n")
                f.write("## ğŸ“ å¾®åšå†…å®¹\n\n")
                
                for i, weibo in enumerate(weibos, 1):
                    f.write(f"### ç¬¬ {i} æ¡å¾®åš\n\n")
                    
                    # åŸºæœ¬ä¿¡æ¯
                    f.write(f"**ğŸ•’ å‘å¸ƒæ—¶é—´**: {weibo.get('formatted_date')}\n")
                    f.write(f"**ğŸ”— å¾®åšé“¾æ¥**: {weibo.get('url')}\n")
                    f.write(f"**ğŸ†” å¾®åšID**: {weibo.get('id')}\n\n")
                    
                    # å®Œæ•´å†…å®¹
                    text = weibo.get('text', '').strip()
                    if text:
                        f.write(f"**ğŸ“„ å®Œæ•´å†…å®¹**:\n\n{text}\n\n")
                    
                    # è½¬å‘å†…å®¹
                    if 'retweeted' in weibo:
                        rt = weibo['retweeted']
                        f.write(f"**ğŸ”„ è½¬å‘å†…å®¹**:\n")
                        f.write(f"> **@{rt.get('user_name', '')}**: {rt.get('text', '')}\n\n")
                    
                    # å›¾ç‰‡
                    if 'images' in weibo and weibo['images']:
                        f.write(f"**ğŸ–¼ï¸ å›¾ç‰‡** ({len(weibo['images'])} å¼ ):\n\n")
                        for idx, img in enumerate(weibo['images'], 1):
                            f.write(f"{idx}. **åŸé“¾æ¥**: {img['url']}\n")
                            if img.get('local_file'):
                                f.write(f"   **æœ¬åœ°æ–‡ä»¶**: `../images/{img['local_file']}`\n")
                                # å¦‚æœæ˜¯Markdownæ”¯æŒçš„å›¾ç‰‡æ ¼å¼ï¼Œç›´æ¥åµŒå…¥
                                if img['local_file'].lower().endswith(('.jpg', '.jpeg', '.png', '.gif')):
                                    f.write(f"   ![å›¾ç‰‡{idx}](../images/{img['local_file']})\n")
                            if img.get('from_retweet'):
                                f.write(f"   *ï¼ˆæ¥è‡ªè½¬å‘å†…å®¹ï¼‰*\n")
                            f.write("\n")
                    
                    # äº’åŠ¨æ•°æ®
                    f.write(f"**ğŸ“Š äº’åŠ¨æ•°æ®**:\n")
                    f.write(f"- ğŸ”„ è½¬å‘: {weibo.get('reposts_count', 0):,}\n")
                    f.write(f"- ğŸ’¬ è¯„è®º: {weibo.get('comments_count', 0):,}\n")
                    f.write(f"- â¤ï¸ ç‚¹èµ: {weibo.get('attitudes_count', 0):,}\n")
                    
                    # æ¥æº
                    if weibo.get('source'):
                        f.write(f"- ğŸ“± æ¥æº: {weibo['source']}\n")
                    
                    f.write(f"\n---\n\n")
            
            else:
                f.write("âš ï¸ æœªè·å–åˆ°å¾®åšå†…å®¹\n")
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: reports/{filename}")
        return filename

    def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        print("ğŸš€ å¯åŠ¨ç»„ç»‡åŒ–å¾®åšçˆ¬è™«...")
        print("ğŸ¯ åŠŸèƒ½: å…¨æ–‡+å›¾ç‰‡ä¸‹è½½+å¾®åšé“¾æ¥+ç›®å½•ç»„ç»‡")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_base}/")
        print("=" * 60)
        
        weibos = self.scrape_all_pages()
        
        print(f"\nğŸ“Š çˆ¬å–å®Œæˆ: {len(weibos)} æ¡å¾®åš")
        
        if weibos:
            print("\nğŸ’¾ ä¿å­˜æ•°æ®æ–‡ä»¶...")
            
            # ä¿å­˜JSONæ•°æ®
            json_filename = f"weibo_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            self.save_data_json(weibos, json_filename)
            
            # ç”ŸæˆMarkdownæŠ¥å‘Š
            report_filename = self.generate_complete_markdown(weibos)
            
            # ç»Ÿè®¡
            total_images = sum(len(w.get('images', [])) for w in weibos)
            
            print(f"\nâœ… ä»»åŠ¡å®Œæˆ!")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_base}/")
            print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: reports/{report_filename.split('/')[-1]}")
            print(f"ğŸ’¾ æ•°æ®æ–‡ä»¶: data/{json_filename}")
            print(f"ğŸ“Š å¾®åšæ•°é‡: {len(weibos)} æ¡")
            print(f"ğŸ–¼ï¸ å›¾ç‰‡ä¸‹è½½: {total_images} å¼  (ä¿å­˜åœ¨ images/ ç›®å½•)")
            
            return {
                'output_dir': self.output_base,
                'report_file': report_filename,
                'data_file': os.path.join(self.data_dir, json_filename),
                'weibo_count': len(weibos),
                'image_count': total_images
            }
        else:
            print("âŒ æœªè·å–åˆ°å¾®åšå†…å®¹")
            return None


def main():
    # å¯ä»¥è‡ªå®šä¹‰è¾“å‡ºç›®å½•åç§°
    scraper = OrganizedWeiboScraper("weibo_output")
    result = scraper.run()
    return result


if __name__ == "__main__":
    main()